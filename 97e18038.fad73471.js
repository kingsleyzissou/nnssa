(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{116:function(e,t,r){"use strict";r.r(t),r.d(t,"frontMatter",(function(){return i})),r.d(t,"metadata",(function(){return s})),r.d(t,"toc",(function(){return c})),r.d(t,"default",(function(){return u}));var n=r(3),a=r(7),o=(r(0),r(143)),i={id:"abstract",title:"Abstract",sidebar_label:"Abstract",slug:"/abstract"},s={unversionedId:"abstract",id:"version-final-report/abstract",isDocsHomePage:!1,title:"Abstract",description:"A web application to identify song sections (i.e. intro, verse, chorus) from uploaded music files. The area of research is Boundary detection detection and song segmentation, a sub-field of music information retrieval. An artificial neural network was developed to detect the boundaries between song sections, using a dataset of 2000 songs. Sections were identified with an F-score accuracy of 70.8% (at the time of going to press) within a specified musical scope of pop and dance, outperforming the state-of-the-art model by 9%. Data was cleaned and pre-processed incrementally in order to optimise the training of the neural network model. A number of approaches were used to subdivide the data, including frames, beats and bars. An automated process was created for identifying the optimal hyperparameters, such as the number of hidden layers, neurons and epochs (the number of times the entire data is passed forward and back again through the layers of the model). The hyperparameter tuning was performed in Google Colab with TPU power. The web application was created with a React frontend. The model was deployed in the cloud, leveraging AWS Lambda and S3 buckets for a serverless, event-driven architecture.",source:"@site/versioned_docs/version-final-report/abstract.md",slug:"/abstract",permalink:"/docs/abstract",editUrl:"https://github.com/kingsleyzissou/nnssa/edit/master/website/versioned_docs/version-final-report/abstract.md",version:"final-report",sidebar_label:"Abstract",sidebar:"version-final-report/someSidebar",previous:{title:"Introduction",permalink:"/docs/"},next:{title:"Music",permalink:"/docs/music"}},c=[],d={toc:c};function u(e){var t=e.components,r=Object(a.a)(e,["components"]);return Object(o.b)("wrapper",Object(n.a)({},d,r,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"A web application to identify song sections (i.e. intro, verse, chorus) from uploaded music files. The area of research is Boundary detection detection and song segmentation, a sub-field of music information retrieval. An artificial neural network was developed to detect the boundaries between song sections, using a dataset of 2000 songs. Sections were identified with an F-score accuracy of 70.8% (at the time of going to press) within a specified musical scope of pop and dance, outperforming the state-of-the-art model by 9%. Data was cleaned and pre-processed incrementally in order to optimise the training of the neural network model. A number of approaches were used to subdivide the data, including frames, beats and bars. An automated process was created for identifying the optimal hyperparameters, such as the number of hidden layers, neurons and epochs (the number of times the entire data is passed forward and back again through the layers of the model). The hyperparameter tuning was performed in Google Colab with TPU power. The web application was created with a React frontend. The model was deployed in the cloud, leveraging AWS Lambda and S3 buckets for a serverless, event-driven architecture."))}u.isMDXComponent=!0},143:function(e,t,r){"use strict";r.d(t,"a",(function(){return l})),r.d(t,"b",(function(){return m}));var n=r(0),a=r.n(n);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function c(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var d=a.a.createContext({}),u=function(e){var t=a.a.useContext(d),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},l=function(e){var t=u(e.components);return a.a.createElement(d.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.a.createElement(a.a.Fragment,{},t)}},f=a.a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,i=e.parentName,d=c(e,["components","mdxType","originalType","parentName"]),l=u(r),f=n,m=l["".concat(i,".").concat(f)]||l[f]||p[f]||o;return r?a.a.createElement(m,s(s({ref:t},d),{},{components:r})):a.a.createElement(m,s({ref:t},d))}));function m(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=f;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:n,i[1]=s;for(var d=2;d<o;d++)i[d]=r[d];return a.a.createElement.apply(null,i)}return a.a.createElement.apply(null,r)}f.displayName="MDXCreateElement"}}]);