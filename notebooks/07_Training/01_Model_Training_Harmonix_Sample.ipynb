{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    ROOT = '/content/drive/MyDrive/fyp/collabs/'\n",
    "else:\n",
    "    ROOT = os.path.join(os.getcwd(), '..', '..') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Colab libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !pip install pickle5\n",
    "    import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load custom modules"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install git+https://github.com/kingsleyzissou/nnssa.git@0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnssa.constants import *\n",
    "from nnssa.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TensorFlow modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, BatchNormalization, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics():\n",
    "    return [\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.15),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('COLAB_TPU_ADDR' in os.environ.keys()):\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "    tf.config.experimental_connect_to_host(resolver.master())\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress bars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Sub_Divisions</th>\n",
       "      <th>Binary_Labels</th>\n",
       "      <th>Weighted_Labels</th>\n",
       "      <th>Weights</th>\n",
       "      <th>IDS</th>\n",
       "      <th>Beat_times</th>\n",
       "      <th>Labels</th>\n",
       "      <th>BPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_12step</td>\n",
       "      <td>bars/harmonix/0001_12step.npy</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[3.0, 0.5, 1.0, 0.5, 3.0, 0.5, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0001_12step, 0001_12step, 0001_12step, 0001_1...</td>\n",
       "      <td>[0.0, 0.5309729999999999, 1.0619459999999998, ...</td>\n",
       "      <td>[0.0, 8.495567999999999, 25.486704, 42.4753280...</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003_6foot7foot</td>\n",
       "      <td>bars/harmonix/0003_6foot7foot.npy</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[3.0, 0.5, 3.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0003_6foot7foot, 0003_6foot7foot, 0003_6foot7...</td>\n",
       "      <td>[2.857108, 3.571394, 4.28568, 4.99996600000000...</td>\n",
       "      <td>[2.857108, 8.571396, 31.428548, 37.14283599999...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004_abc</td>\n",
       "      <td>bars/harmonix/0004_abc.npy</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[3.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0004_abc, 0004_abc, 0004_abc, 0004_abc, 0004_...</td>\n",
       "      <td>[2.666656, 3.238084, 3.952369, 4.597529, 5.242...</td>\n",
       "      <td>[2.666656, 28.300542999999998, 58.263180000000...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006_aint2proud2beg</td>\n",
       "      <td>bars/harmonix/0006_aint2proud2beg.npy</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[3.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0006_aint2proud2beg, 0006_aint2proud2beg, 000...</td>\n",
       "      <td>[0.0, 0.572203, 1.144406, 1.716609, 2.288812, ...</td>\n",
       "      <td>[0.0, 27.4652, 45.203726, 63.518522999999995, ...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008_america</td>\n",
       "      <td>bars/harmonix/0008_america.npy</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[3.0, 0.5, 1.0, 0.5, 3.0, 0.5, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0008_america, 0008_america, 0008_america, 000...</td>\n",
       "      <td>[3.871208, 4.359011, 4.846814, 5.338616, 5.830...</td>\n",
       "      <td>[3.871208, 10.56504, 33.217138, 56.85190400000...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  File                          Sub_Divisions  \\\n",
       "0          0001_12step          bars/harmonix/0001_12step.npy   \n",
       "1      0003_6foot7foot      bars/harmonix/0003_6foot7foot.npy   \n",
       "2             0004_abc             bars/harmonix/0004_abc.npy   \n",
       "3  0006_aint2proud2beg  bars/harmonix/0006_aint2proud2beg.npy   \n",
       "4         0008_america         bars/harmonix/0008_america.npy   \n",
       "\n",
       "                                       Binary_Labels  \\\n",
       "0  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                     Weighted_Labels  \\\n",
       "0  [1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                             Weights  \\\n",
       "0  [3.0, 0.5, 1.0, 0.5, 3.0, 0.5, 1.0, 1.0, 1.0, ...   \n",
       "1  [3.0, 0.5, 3.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "2  [3.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "3  [3.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "4  [3.0, 0.5, 1.0, 0.5, 3.0, 0.5, 1.0, 1.0, 1.0, ...   \n",
       "\n",
       "                                                 IDS  \\\n",
       "0  [0001_12step, 0001_12step, 0001_12step, 0001_1...   \n",
       "1  [0003_6foot7foot, 0003_6foot7foot, 0003_6foot7...   \n",
       "2  [0004_abc, 0004_abc, 0004_abc, 0004_abc, 0004_...   \n",
       "3  [0006_aint2proud2beg, 0006_aint2proud2beg, 000...   \n",
       "4  [0008_america, 0008_america, 0008_america, 000...   \n",
       "\n",
       "                                          Beat_times  \\\n",
       "0  [0.0, 0.5309729999999999, 1.0619459999999998, ...   \n",
       "1  [2.857108, 3.571394, 4.28568, 4.99996600000000...   \n",
       "2  [2.666656, 3.238084, 3.952369, 4.597529, 5.242...   \n",
       "3  [0.0, 0.572203, 1.144406, 1.716609, 2.288812, ...   \n",
       "4  [3.871208, 4.359011, 4.846814, 5.338616, 5.830...   \n",
       "\n",
       "                                              Labels  BPM  \n",
       "0  [0.0, 8.495567999999999, 25.486704, 42.4753280...  113  \n",
       "1  [2.857108, 8.571396, 31.428548, 37.14283599999...   84  \n",
       "2  [2.666656, 28.300542999999998, 58.263180000000...   94  \n",
       "3  [0.0, 27.4652, 45.203726, 63.518522999999995, ...  105  \n",
       "4  [3.871208, 10.56504, 33.217138, 56.85190400000...  136  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if COLAB:\n",
    "    harmonix_beats = pickle.load(open(os.path.join(ROOT, SUB_DIVS_DIR, 'bars', 'harmonix.p'), 'rb'))\n",
    "else:\n",
    "    harmonix_beats = pd.read_pickle(os.path.join(ROOT, SUB_DIVS_DIR, 'bars', 'harmonix.p'))\n",
    "harmonix_beats = harmonix_beats.head(100)\n",
    "harmonix_beats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mel Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_np(file):\n",
    "    return np.load(os.path.join(ROOT, SUB_DIVS_DIR, file), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 65.43it/s]\n"
     ]
    }
   ],
   "source": [
    "harmonix_beats['Sub_Divisions'] = harmonix_beats['Sub_Divisions'].progress_map(load_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonix_beats['Oversamples'] = harmonix_beats['Oversamples'].progress_map(load_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = harmonix_beats.head(50).copy()\n",
    "y = harmonix_beats.head(50)['Binary_Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, _, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, _, _ = train_test_split(X_train, X_train['Binary_Labels'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.concatenate(X_train['Over_Labels'].values)\n",
    "y_train = np.concatenate(X_train['Binary_Labels'].values)\n",
    "y_test = np.concatenate(X_test['Binary_Labels'].values)\n",
    "y_val = np.concatenate(X_val['Binary_Labels'].values)\n",
    "\n",
    "ids_test = np.concatenate(X_test['IDS'].values)\n",
    "\n",
    "# w_train = np.concatenate(X_train['Over_Weights'].values)\n",
    "w_train = np.concatenate(X_train['Weights'].values)\n",
    "w_test = np.concatenate(X_test['Weights'].values)\n",
    "w_val = np.concatenate(X_val['Weights'].values)\n",
    "\n",
    "# X_train = np.concatenate(X_train['Oversamples'].values)\n",
    "X_train = np.concatenate(X_train['Sub_Divisions'].values)\n",
    "X_test = np.concatenate(X_test['Sub_Divisions'].values)\n",
    "X_val = np.concatenate(X_val['Sub_Divisions'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 273, 0.0: 2995})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Initial Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial bias: [-2.39522772]\n"
     ]
    }
   ],
   "source": [
    "count = np.bincount(y_train.astype('int64'))\n",
    "neg, pos = count[0], count[1]\n",
    "total = neg + pos\n",
    "initial_bias = np.log([pos/neg])\n",
    "print(f'Initial bias: {initial_bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(initial_bias):\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    bias_initializer = tf.keras.initializers.Constant(initial_bias)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    return Sequential([\n",
    "        Input(shape=(N_MELS, 4, 33)),\n",
    "        Conv2D(8, 8, activation='relu', kernel_initializer=initializer, padding=\"same\"),\n",
    "        MaxPooling2D(pool_size=(5, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Conv2D(16, 6, activation='relu', padding=\"same\", kernel_initializer=initializer),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='sigmoid'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid', bias_initializer=bias_initializer)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3038\n",
      "Epoch 1/80\n",
      "26/26 [==============================] - 6s 170ms/step - loss: 0.6401 - accuracy: 0.3862 - precision: 0.1207 - recall: 0.0769 - auc: 0.5246 - val_loss: 0.4217 - val_accuracy: 0.0833 - val_precision: 0.2941 - val_recall: 0.0769 - val_auc: 0.6281\n",
      "Epoch 2/80\n",
      "26/26 [==============================] - 3s 135ms/step - loss: 0.5873 - accuracy: 0.4159 - precision: 0.1875 - recall: 0.1099 - auc: 0.6091 - val_loss: 0.6710 - val_accuracy: 0.0833 - val_precision: 0.1405 - val_recall: 0.8000 - val_auc: 0.7667\n",
      "Epoch 3/80\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.5294 - accuracy: 0.5073 - precision: 0.2250 - recall: 0.1978 - auc: 0.7204 - val_loss: 0.7707 - val_accuracy: 0.1487 - val_precision: 0.1341 - val_recall: 0.9385 - val_auc: 0.8057\n",
      "Epoch 4/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.5299 - accuracy: 0.5786 - precision: 0.2577 - recall: 0.2747 - auc: 0.7269 - val_loss: 1.0188 - val_accuracy: 0.0885 - val_precision: 0.1140 - val_recall: 1.0000 - val_auc: 0.8039\n",
      "Epoch 5/80\n",
      "26/26 [==============================] - 4s 136ms/step - loss: 0.4880 - accuracy: 0.6138 - precision: 0.2874 - recall: 0.3590 - auc: 0.7777 - val_loss: 0.8005 - val_accuracy: 0.2321 - val_precision: 0.1485 - val_recall: 0.9231 - val_auc: 0.8439\n",
      "Epoch 6/80\n",
      "26/26 [==============================] - 3s 133ms/step - loss: 0.4493 - accuracy: 0.6631 - precision: 0.3333 - recall: 0.4725 - auc: 0.8182 - val_loss: 0.4096 - val_accuracy: 0.5128 - val_precision: 0.2199 - val_recall: 0.6462 - val_auc: 0.8285\n",
      "Epoch 7/80\n",
      "26/26 [==============================] - 4s 136ms/step - loss: 0.4380 - accuracy: 0.6763 - precision: 0.3455 - recall: 0.4505 - auc: 0.8292 - val_loss: 0.3597 - val_accuracy: 0.6462 - val_precision: 0.2237 - val_recall: 0.5231 - val_auc: 0.8117\n",
      "Epoch 8/80\n",
      "26/26 [==============================] - 4s 142ms/step - loss: 0.4086 - accuracy: 0.7283 - precision: 0.3614 - recall: 0.5495 - auc: 0.8589 - val_loss: 0.3117 - val_accuracy: 0.7205 - val_precision: 0.3182 - val_recall: 0.6462 - val_auc: 0.8555\n",
      "Epoch 9/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.3804 - accuracy: 0.7537 - precision: 0.4015 - recall: 0.5971 - auc: 0.8802 - val_loss: 0.3136 - val_accuracy: 0.6782 - val_precision: 0.3250 - val_recall: 0.6000 - val_auc: 0.8483\n",
      "Epoch 10/80\n",
      "26/26 [==============================] - 4s 135ms/step - loss: 0.3608 - accuracy: 0.7607 - precision: 0.4367 - recall: 0.6190 - auc: 0.8930 - val_loss: 0.2988 - val_accuracy: 0.7615 - val_precision: 0.3545 - val_recall: 0.6000 - val_auc: 0.8574\n",
      "Epoch 11/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.3605 - accuracy: 0.7573 - precision: 0.4296 - recall: 0.6813 - auc: 0.8926 - val_loss: 0.2975 - val_accuracy: 0.8205 - val_precision: 0.3529 - val_recall: 0.3692 - val_auc: 0.7916\n",
      "Epoch 12/80\n",
      "26/26 [==============================] - 4s 135ms/step - loss: 0.3549 - accuracy: 0.7674 - precision: 0.4378 - recall: 0.6447 - auc: 0.8938 - val_loss: 0.2658 - val_accuracy: 0.8205 - val_precision: 0.3902 - val_recall: 0.4923 - val_auc: 0.8507\n",
      "Epoch 13/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.3250 - accuracy: 0.7922 - precision: 0.4476 - recall: 0.6410 - auc: 0.9170 - val_loss: 0.2585 - val_accuracy: 0.8474 - val_precision: 0.4459 - val_recall: 0.5077 - val_auc: 0.8668\n",
      "Epoch 14/80\n",
      "26/26 [==============================] - 4s 135ms/step - loss: 0.3303 - accuracy: 0.7922 - precision: 0.4572 - recall: 0.6850 - auc: 0.9126 - val_loss: 0.3759 - val_accuracy: 0.7974 - val_precision: 0.2755 - val_recall: 0.4154 - val_auc: 0.7399\n",
      "Epoch 15/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.3103 - accuracy: 0.8063 - precision: 0.4797 - recall: 0.6923 - auc: 0.9202 - val_loss: 0.2534 - val_accuracy: 0.8692 - val_precision: 0.5000 - val_recall: 0.4154 - val_auc: 0.8464\n",
      "Epoch 16/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.2760 - accuracy: 0.8237 - precision: 0.5230 - recall: 0.7509 - auc: 0.9386 - val_loss: 0.2749 - val_accuracy: 0.8641 - val_precision: 0.4062 - val_recall: 0.4000 - val_auc: 0.8282\n",
      "Epoch 17/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.2529 - accuracy: 0.8436 - precision: 0.5212 - recall: 0.7656 - auc: 0.9489 - val_loss: 0.3365 - val_accuracy: 0.8346 - val_precision: 0.3415 - val_recall: 0.4308 - val_auc: 0.8121\n",
      "Epoch 18/80\n",
      "26/26 [==============================] - 4s 135ms/step - loss: 0.2735 - accuracy: 0.8311 - precision: 0.5315 - recall: 0.7729 - auc: 0.9391 - val_loss: 0.2935 - val_accuracy: 0.8474 - val_precision: 0.3827 - val_recall: 0.4769 - val_auc: 0.8487\n",
      "Epoch 19/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.2639 - accuracy: 0.8415 - precision: 0.5037 - recall: 0.7473 - auc: 0.9441 - val_loss: 0.3932 - val_accuracy: 0.8128 - val_precision: 0.2887 - val_recall: 0.4308 - val_auc: 0.7988\n",
      "Epoch 20/80\n",
      "26/26 [==============================] - 4s 139ms/step - loss: 0.2525 - accuracy: 0.8412 - precision: 0.5170 - recall: 0.7802 - auc: 0.9494 - val_loss: 0.3079 - val_accuracy: 0.8962 - val_precision: 0.4286 - val_recall: 0.3231 - val_auc: 0.7932\n",
      "Epoch 21/80\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.3094 - accuracy: 0.8216 - precision: 0.4739 - recall: 0.7326 - auc: 0.9210 - val_loss: 0.3170 - val_accuracy: 0.7936 - val_precision: 0.3021 - val_recall: 0.4462 - val_auc: 0.8075\n",
      "Epoch 22/80\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.2426 - accuracy: 0.8449 - precision: 0.5541 - recall: 0.7692 - auc: 0.9531 - val_loss: 0.2639 - val_accuracy: 0.8808 - val_precision: 0.4407 - val_recall: 0.4000 - val_auc: 0.8654\n",
      "Epoch 23/80\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.2294 - accuracy: 0.8767 - precision: 0.5968 - recall: 0.8132 - auc: 0.9578 - val_loss: 0.3054 - val_accuracy: 0.8731 - val_precision: 0.3380 - val_recall: 0.3692 - val_auc: 0.8568\n",
      "Epoch 24/80\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.2666 - accuracy: 0.8387 - precision: 0.4894 - recall: 0.7582 - auc: 0.9448 - val_loss: 0.3293 - val_accuracy: 0.8449 - val_precision: 0.3220 - val_recall: 0.2923 - val_auc: 0.7301\n",
      "Epoch 25/80\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.2506 - accuracy: 0.8516 - precision: 0.5414 - recall: 0.7912 - auc: 0.9482 - val_loss: 0.4053 - val_accuracy: 0.7949 - val_precision: 0.2479 - val_recall: 0.4462 - val_auc: 0.7891\n",
      "Epoch 26/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.2310 - accuracy: 0.8562 - precision: 0.5504 - recall: 0.8205 - auc: 0.9562 - val_loss: 0.2941 - val_accuracy: 0.8692 - val_precision: 0.3500 - val_recall: 0.3231 - val_auc: 0.8415\n",
      "Epoch 27/80\n",
      "26/26 [==============================] - 4s 139ms/step - loss: 0.2316 - accuracy: 0.8687 - precision: 0.5805 - recall: 0.8059 - auc: 0.9550 - val_loss: 0.2707 - val_accuracy: 0.8885 - val_precision: 0.4500 - val_recall: 0.4154 - val_auc: 0.8573\n",
      "Epoch 28/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.1946 - accuracy: 0.8764 - precision: 0.5844 - recall: 0.8242 - auc: 0.9709 - val_loss: 0.3144 - val_accuracy: 0.8923 - val_precision: 0.3636 - val_recall: 0.2462 - val_auc: 0.8123\n",
      "Epoch 29/80\n",
      "26/26 [==============================] - 4s 136ms/step - loss: 0.1681 - accuracy: 0.9009 - precision: 0.6456 - recall: 0.8608 - auc: 0.9776 - val_loss: 0.3360 - val_accuracy: 0.8744 - val_precision: 0.4194 - val_recall: 0.4000 - val_auc: 0.8256\n",
      "Epoch 30/80\n",
      "26/26 [==============================] - 4s 139ms/step - loss: 0.2278 - accuracy: 0.8853 - precision: 0.5646 - recall: 0.8168 - auc: 0.9593 - val_loss: 0.4916 - val_accuracy: 0.8256 - val_precision: 0.2667 - val_recall: 0.4308 - val_auc: 0.7465\n",
      "Epoch 31/80\n",
      "26/26 [==============================] - 4s 137ms/step - loss: 0.2005 - accuracy: 0.8819 - precision: 0.5938 - recall: 0.8462 - auc: 0.9668 - val_loss: 0.4764 - val_accuracy: 0.8051 - val_precision: 0.2605 - val_recall: 0.4769 - val_auc: 0.7757\n",
      "Epoch 32/80\n",
      "26/26 [==============================] - 4s 136ms/step - loss: 0.1977 - accuracy: 0.8831 - precision: 0.6059 - recall: 0.8278 - auc: 0.9678 - val_loss: 0.3404 - val_accuracy: 0.8513 - val_precision: 0.4051 - val_recall: 0.4923 - val_auc: 0.8130\n",
      "Epoch 33/80\n",
      "26/26 [==============================] - 4s 138ms/step - loss: 0.1953 - accuracy: 0.8819 - precision: 0.6146 - recall: 0.8645 - auc: 0.9668 - val_loss: 0.3227 - val_accuracy: 0.8679 - val_precision: 0.3519 - val_recall: 0.2923 - val_auc: 0.8202\n"
     ]
    }
   ],
   "source": [
    "model = build_model(initial_bias)\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.05, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=get_metrics())\n",
    "\n",
    "results = model.evaluate(X_test, y_test, batch_size=32, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=128,\n",
    "    epochs=80, \n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    sample_weight=w_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972/972 [==============================] - 1s 1ms/step\n",
      "31/31 [==============================] - 0s 8ms/step - loss: 0.2569 - accuracy: 0.8611 - precision: 0.5000 - recall: 0.4483 - auc: 0.8540\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test, batch_size=1, verbose=1)\n",
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.95       885\n",
      "         1.0       0.50      0.45      0.47        87\n",
      "\n",
      "    accuracy                           0.91       972\n",
      "   macro avg       0.72      0.70      0.71       972\n",
      "weighted avg       0.91      0.91      0.91       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [1 if (p > 0.5) else 0 for p in preds]\n",
    "y_pred = np.asarray(y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:  0.7079411816253922\n",
      "Precision:  0.7491938013677144\n",
      "Recall:  0.7204761904761905\n"
     ]
    }
   ],
   "source": [
    "f_score, precision, recall = evaluate(harmonix_beats, preds, ids_test, True)\n",
    "\n",
    "print(\"F-score: \", f_score)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best score\n",
    " * Every iteration of this notebook has a different result\n",
    " * The best score achieved is reported below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "3 seconds:\n",
    "F-score:  0.7079411816253922\n",
    "Precision:  0.7491938013677144\n",
    "Recall:  0.7204761904761905\n",
    "\n",
    "0.5 seconds:\n",
    "F-score:  0.3044451156585869\n",
    "Precision:  0.2801110180142438\n",
    "Recall:  0.35031746031746025\n",
    "\n",
    "2 bars:\n",
    "F-score:  0.7391024180497865\n",
    "Precision:  0.7825271347010477\n",
    "Recall:  0.7499206349206349\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best score architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model = Sequential([\n",
    "    Input(shape=(N_MELS, 4, 33)),\n",
    "    Conv2D(8, 8, activation='relu', kernel_initializer=initializer, padding=\"same\"),\n",
    "    MaxPooling2D(pool_size=(5, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(16, 6, activation='relu', padding=\"same\", kernel_initializer=initializer),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid', bias_initializer=bias_initializer)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save(os.path.join(ROOT, 'models', 'binary_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.savez(os.path.join(ROOT, 'data', '06_Results', 'harmonix.npz'), preds=preds, ids=ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
